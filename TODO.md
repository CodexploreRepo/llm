# TODO

- [ ] [RAG Workshop](https://www.youtube.com/live/bNqSRNMgwhQ?si=4W6FNtODtpa5_log) with slide and code in the description
- [ ] [ArXiv Digest and Personalized Recommendations using Large Language Models](https://github.com/AutoLLM/ArxivDigest)
- [ ] [Fine-tune Llama 2 for sentiment analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis) with [Medium](https://medium.com/@ud.chandra/instruction-fine-tuning-llama-2-with-pefts-qlora-method-d6a801ebb19)
  - Dataset:
    - [Zero Shot Aspect based Sentiment Analysis with Llama 2 7B LLM Langchain](https://youtu.be/LNTK-CLYquA?si=tkCB12tKd2qK_G1x)
    - [Federal Reserve FOMC Speech (2015-2023)](https://www.kaggle.com/code/drlexus/fomc-statement-generation-gpt-2)
    - [Federal Reserve FOMC Speech (1996-2020)](https://www.kaggle.com/datasets/natanm/federal-reserve-governors-speeches-1996-2020)
    - [Speech Scraping](https://www.kaggle.com/code/natanm/speech-scraping)
- [ ] [Low-Rank Adaptation of Large Language Models (LoRA)](<https://huggingface.co/docs/diffusers/training/lora#:~:text=Low%2DRank%20Adaptation%20of%20Large%20Language%20Models%20(LoRA)%20is,trains%20those%20newly%20added%20weights.>)
