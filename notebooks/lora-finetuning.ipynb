{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LORA Finetuning\n## Reference\n- [Fine-Tuning Large Language Models (LLMs)](https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/fine-tuning/ft-example.ipynb)","metadata":{}},{"cell_type":"code","source":"!pip install -q peft evaluate","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:09:12.942796Z","iopub.execute_input":"2023-12-31T10:09:12.943104Z","iopub.status.idle":"2023-12-31T10:09:27.240973Z","shell.execute_reply.started":"2023-12-31T10:09:12.943078Z","shell.execute_reply":"2023-12-31T10:09:27.239935Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !pip install --ignore-installed -q datasets==2.15 # upgrade from datasets 2.10","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:09:27.243034Z","iopub.execute_input":"2023-12-31T10:09:27.243330Z","iopub.status.idle":"2023-12-31T10:09:27.247570Z","shell.execute_reply.started":"2023-12-31T10:09:27.243305Z","shell.execute_reply":"2023-12-31T10:09:27.246691Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, Dataset\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig, \n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer)\n\n# Parameter-Efficient Fine-Tuning (PEFT)\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport evaluate\nimport torch\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:09:27.248777Z","iopub.execute_input":"2023-12-31T10:09:27.249105Z","iopub.status.idle":"2023-12-31T10:09:45.665710Z","shell.execute_reply.started":"2023-12-31T10:09:27.249075Z","shell.execute_reply":"2023-12-31T10:09:45.664944Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:09:45.667213Z","iopub.execute_input":"2023-12-31T10:09:45.667807Z","iopub.status.idle":"2023-12-31T10:09:45.697710Z","shell.execute_reply.started":"2023-12-31T10:09:45.667778Z","shell.execute_reply":"2023-12-31T10:09:45.696477Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset ","metadata":{}},{"cell_type":"markdown","source":"- Refer: [Dataset slice split to get more information about download dataset from HuggingFace](https://huggingface.co/docs/datasets/loading#slice-splits)","metadata":{}},{"cell_type":"code","source":"split=\"train\"\n\ndataset = load_dataset(\"imdb\", split=split)\n\n# 70% train, 15% test + 15%validation\ntrain_testvalid = dataset.train_test_split(test_size=0.3)\n\n# Split the 10% test + valid in half test, half valid\ntest_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n\n# gather everyone if you want to have a single DatasetDict\ndataset = DatasetDict({'train': train_testvalid['train'], 'test': test_valid['test'], 'validation': test_valid['train']})","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:27:00.427749Z","iopub.execute_input":"2023-12-31T10:27:00.428196Z","iopub.status.idle":"2023-12-31T10:27:00.713189Z","shell.execute_reply.started":"2023-12-31T10:27:00.428161Z","shell.execute_reply":"2023-12-31T10:27:00.712168Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# display % of training data with label=1\nnp.array(dataset['train']['label']).sum()/len(dataset['train']['label'])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:27:04.009260Z","iopub.execute_input":"2023-12-31T10:27:04.010211Z","iopub.status.idle":"2023-12-31T10:27:04.259745Z","shell.execute_reply.started":"2023-12-31T10:27:04.010175Z","shell.execute_reply":"2023-12-31T10:27:04.258614Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"0.4997714285714286"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"model_checkpoint = 'distilbert-base-uncased'\n# model_checkpoint = 'roberta-base' # you can alternatively use roberta-base but this model is bigger thus training will take longer\n\n# define label maps\nid2label = {0: \"Negative\", 1: \"Positive\"}\nlabel2id = {\"Negative\":0, \"Positive\":1}\n\n# generate classification model from model_checkpoint\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:27:12.511994Z","iopub.execute_input":"2023-12-31T10:27:12.512767Z","iopub.status.idle":"2023-12-31T10:27:12.767880Z","shell.execute_reply.started":"2023-12-31T10:27:12.512731Z","shell.execute_reply":"2023-12-31T10:27:12.766755Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokeniser","metadata":{}},{"cell_type":"code","source":"# create tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n\n# add pad token if none exists\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:27:19.336515Z","iopub.execute_input":"2023-12-31T10:27:19.337138Z","iopub.status.idle":"2023-12-31T10:27:19.449318Z","shell.execute_reply.started":"2023-12-31T10:27:19.337103Z","shell.execute_reply":"2023-12-31T10:27:19.448176Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# create tokenize function\ndef tokenize_function(examples):\n    # extract text\n    text = examples[\"text\"]\n\n    #tokenize and truncate text\n    tokenizer.truncation_side = \"left\"\n    tokenized_inputs = tokenizer(\n        text,\n        return_tensors=\"np\",\n        truncation=True,\n        max_length=512\n    )\n\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:10:33.522476Z","iopub.execute_input":"2023-12-31T10:10:33.522769Z","iopub.status.idle":"2023-12-31T10:10:33.527861Z","shell.execute_reply.started":"2023-12-31T10:10:33.522744Z","shell.execute_reply":"2023-12-31T10:10:33.526943Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# tokenize training and validation datasets\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\n# create data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:27:22.618236Z","iopub.execute_input":"2023-12-31T10:27:22.618647Z","iopub.status.idle":"2023-12-31T10:27:34.986852Z","shell.execute_reply.started":"2023-12-31T10:27:22.618612Z","shell.execute_reply":"2023-12-31T10:27:34.985749Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b6a891d03d24e8db575bc895516c259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f7a8601eb643859e80fabddf3f38f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a9a9fe8fcc64e0da2b9328035471521"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# import accuracy evaluation metric\naccuracy = evaluate.load(\"accuracy\")\n# define an evaluation function to pass into trainer later\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=1)\n\n    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:27:40.035494Z","iopub.execute_input":"2023-12-31T10:27:40.035884Z","iopub.status.idle":"2023-12-31T10:27:40.397533Z","shell.execute_reply.started":"2023-12-31T10:27:40.035847Z","shell.execute_reply":"2023-12-31T10:27:40.396389Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Untrained model performance\n- Before training our model, we can evaluate how the base model with a randomly initialized classification head performs on some example inp","metadata":{}},{"cell_type":"code","source":"# Apply \"untrained\" model to text\n\n# define list of examples\ntext_list = [\"It was good.\", \"Not a fan, don't recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\n\nprint(\"Untrained model predictions:\")\nprint(\"----------------------------\")\nfor text in text_list:\n    # tokenize text\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n    # compute logits\n    logits = model(inputs).logits\n    # convert logits to label\n    predictions = torch.argmax(logits)\n\n    print(f\"{text:40}: {id2label[predictions.tolist()]}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:27:43.943657Z","iopub.execute_input":"2023-12-31T10:27:43.944318Z","iopub.status.idle":"2023-12-31T10:27:44.114560Z","shell.execute_reply.started":"2023-12-31T10:27:43.944277Z","shell.execute_reply":"2023-12-31T10:27:44.112529Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Untrained model predictions:\n----------------------------\nIt was good.                            : Positive\nNot a fan, don't recommed.              : Negative\nBetter than the first one.              : Positive\nThis is not worth watching even once.   : Positive\nThis one is a pass.                     : Negative\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification\n                        r=4, # intrinsic rank of trainable weight matrix\n                        lora_alpha=32, # this is like a learning rate\n                        lora_dropout=0.01, # probablity of dropout\n                        target_modules = ['q_lin']) # we apply lora to query layer only","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:28:14.395470Z","iopub.execute_input":"2023-12-31T10:28:14.396305Z","iopub.status.idle":"2023-12-31T10:28:14.402223Z","shell.execute_reply.started":"2023-12-31T10:28:14.396265Z","shell.execute_reply":"2023-12-31T10:28:14.401200Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# new version of our model that can be trained via PEFT.\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:28:16.247132Z","iopub.execute_input":"2023-12-31T10:28:16.247470Z","iopub.status.idle":"2023-12-31T10:28:16.276222Z","shell.execute_reply.started":"2023-12-31T10:28:16.247445Z","shell.execute_reply":"2023-12-31T10:28:16.274578Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9306847223789819\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- define hyperparameters for model training","metadata":{}},{"cell_type":"code","source":"# hyperparameters\nlr = 1e-3 # size of optimization step \nbatch_size = 32 # number of examples processed per optimziation step\nnum_epochs = 5 # number of times model runs through training data\n\n# define training arguments\ntraining_args = TrainingArguments(\n    output_dir= model_checkpoint + \"-lora-text-classification\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:28:26.407592Z","iopub.execute_input":"2023-12-31T10:28:26.408412Z","iopub.status.idle":"2023-12-31T10:28:26.418770Z","shell.execute_reply.started":"2023-12-31T10:28:26.408376Z","shell.execute_reply":"2023-12-31T10:28:26.417724Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"- How to clear GPU memory without stopping the GPU runtime\n- \"!nvidia-smi\" inside a cell in the notebook, and kill the process id for the GPU like \"!kill process_id\" run the command\n- Tensorflow:\n\n```python\n# !pip install numba\n\nfrom numba import cuda \ndevice = cuda.get_current_device()\ndevice.reset()\n```\n\n- Pytorch:  `torch.cuda.clear_cache`\n","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:28:29.143033Z","iopub.execute_input":"2023-12-31T10:28:29.143414Z","iopub.status.idle":"2023-12-31T10:28:29.350226Z","shell.execute_reply.started":"2023-12-31T10:28:29.143369Z","shell.execute_reply":"2023-12-31T10:28:29.349179Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:28:34.416631Z","iopub.execute_input":"2023-12-31T10:28:34.417020Z","iopub.status.idle":"2023-12-31T10:28:35.475444Z","shell.execute_reply.started":"2023-12-31T10:28:34.416987Z","shell.execute_reply":"2023-12-31T10:28:35.474297Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Sun Dec 31 10:28:35 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    32W / 250W |   1531MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# creater trainer object\ntrainer = Trainer(\n    model=peft_model, # our peft model\n    args=training_args, # hyperparameters\n    train_dataset=tokenized_dataset[\"train\"], # training data\n    eval_dataset=tokenized_dataset[\"validation\"], # validation data\n    tokenizer=tokenizer, # define tokenizer\n    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before\n)\n\n# train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:28:38.911095Z","iopub.execute_input":"2023-12-31T10:28:38.911527Z","iopub.status.idle":"2023-12-31T11:00:38.649248Z","shell.execute_reply.started":"2023-12-31T10:28:38.911486Z","shell.execute_reply":"2023-12-31T11:00:38.648148Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2735' max='2735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2735/2735 31:58, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.282700</td>\n      <td>0.229511</td>\n      <td>{'accuracy': 0.9072}</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.225300</td>\n      <td>0.215059</td>\n      <td>{'accuracy': 0.9149333333333334}</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192600</td>\n      <td>0.222459</td>\n      <td>{'accuracy': 0.9154666666666667}</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.164300</td>\n      <td>0.228699</td>\n      <td>{'accuracy': 0.9168}</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.142300</td>\n      <td>0.237962</td>\n      <td>{'accuracy': 0.9149333333333334}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'accuracy': 0.9072}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9149333333333334}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9154666666666667}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9168}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'accuracy': 0.9149333333333334}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2735, training_loss=0.19604631804025152, metrics={'train_runtime': 1919.1478, 'train_samples_per_second': 45.593, 'train_steps_per_second': 1.425, 'total_flos': 1.175572898007168e+16, 'train_loss': 0.19604631804025152, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"peft_model.to(DEVICE)\nprint(\"Trained model predictions:\")\nprint(\"--------------------------\")\nfor text in text_list:\n    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE) # moving to mps for Mac (can alternatively do 'cpu')\n    print(inputs)\n    logits = peft_model(inputs).logits\n    predictions = torch.max(logits,1).indices\n\n    print(text + \" - \" + id2label[predictions.tolist()[0]])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T11:05:51.144235Z","iopub.execute_input":"2023-12-31T11:05:51.144620Z","iopub.status.idle":"2023-12-31T11:05:51.200721Z","shell.execute_reply.started":"2023-12-31T11:05:51.144589Z","shell.execute_reply":"2023-12-31T11:05:51.199866Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Trained model predictions:\n--------------------------\ntensor([[ 101, 2009, 2001, 2204, 1012,  102]], device='cuda:0')\nIt was good. - Positive\ntensor([[  101,  2025,  1037,  5470,  1010,  2123,  1005,  1056, 28667,  5358,\n          7583,  1012,   102]], device='cuda:0')\nNot a fan, don't recommed. - Negative\ntensor([[ 101, 2488, 2084, 1996, 2034, 2028, 1012,  102]], device='cuda:0')\nBetter than the first one. - Positive\ntensor([[ 101, 2023, 2003, 2025, 4276, 3666, 2130, 2320, 1012,  102]],\n       device='cuda:0')\nThis is not worth watching even once. - Positive\ntensor([[ 101, 2023, 2028, 2003, 1037, 3413, 1012,  102]], device='cuda:0')\nThis one is a pass. - Positive\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}