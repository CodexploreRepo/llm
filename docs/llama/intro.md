# Llama Introduction

## Llama 2

<p align="center"><img width=400 src="../../assets/img/llama2-family.png"></p>

<p align="center"><img width=500 src="../../assets/img/llama-benchmark.png"></p>

### How to access Llama models

- There are couple of methods to access the Llama models
  - Hosting API service: Amazon Bedrock, AnyScale, Google Cloud, Microsoft Azure, Replicate, Together.ai, etc
  - Self-configured Cloud: such as self-own hosting in Amazon Sagemaker
  - Local PC

<p align="center"><img width=300 src="../../assets/img/llama2-hosting.png"></p>

### Code Llama

- **Code Llama** is a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.
- Code Llama cover a wide range of applications:
  - Foundation model (Code Llama)
  - Python specialization model (Code Llama - Python)
  - Instruction-following model (Code Llama - Instruct) which is fine-tuned for understanding natural language instructions.

<p align="center"><img width=400 src="../../assets/img/code-llama-family.png"></p>
